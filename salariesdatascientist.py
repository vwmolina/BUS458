# -*- coding: utf-8 -*-
"""SalariesDataScientist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15UnG3QcrjPQa3QGX8waMvbkV1wRPvm0e
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier

import xgboost
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

from sklearn.model_selection import RandomizedSearchCV

df = pd.read_csv('/content/drive/MyDrive/kaggle_survey_2022_responses.csv')
df.head()

df = df.iloc[1:].copy() #Removed first row

df.shape

df.info()

df.isna().sum()

df.describe()

# List of multiple-choice column prefixes
multiple_choice_prefixes = ['Q6', 'Q7', 'Q10', 'Q12', 'Q13', 'Q14', 'Q15', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q28', 'Q31', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40', 'Q41', 'Q42', 'Q44']

for prefix in multiple_choice_prefixes:
    # Get all columns that start with the prefix
    cols = [col for col in df.columns if col.startswith(prefix + '_') or col == prefix]  # Include the base question, in case

    if cols:  # Check if the list is not empty
        df[cols] = df[cols].fillna(0) # Fill NaN with 0
        for col in cols:
            if df[col].dtype == 'object':
                df[col] = df[col].apply(lambda x: 1 if isinstance(x, str) else 0) # Convert to binary


print("Multiple-choice columns encoded and NaNs handled.\n")
df.head()

df.info()

def rename_multiple_choice_columns(df):
    """Renames multiple choice columns in a DataFrame to 'Q#_Answer Choice' format."""
    column_name_mapping = {
        'Q6_1': 'Q6_Coursera',
        'Q6_2': 'Q6_edX',
        'Q6_3': 'Q6_Kaggle Learn Courses',
        'Q6_4': 'Q6_DataCamp',
        'Q6_5': 'Q6_Fast.ai',
        'Q6_6': 'Q6_Udacity',
        'Q6_7': 'Q6_Udemy',
        'Q6_8': 'Q6_LinkedIn Learning',
        'Q6_9': 'Q6_Cloud-certification programs (direct from AWS, Azure, GCP, or similar)',
        'Q6_10': 'Q6_University Courses (resulting in a degree)',
        'Q6_11': 'Q6_None',
        'Q6_12': 'Q6_Other',

        'Q7_1': 'Q7_University Courses',
        'Q7_2': 'Q7_Online courses (Coursera, edX, Kaggle Learn, etc)',
        'Q7_3': 'Q7_Social media platforms (Reddit, Twitter, etc)',
        'Q7_4': 'Q7_Video platforms (YouTube, Vimeo, etc)',
        'Q7_5': 'Q7_Kaggle (notebooks, forums, etc)',
        'Q7_6': 'Q7_None',
        'Q7_7': 'Q7_Other',

        'Q10_1': 'Q10_Yes,theoretical research',
        'Q10_2': 'Q10_Yes,applied research',
        'Q10_3': 'Q10_No',

        'Q12_1': 'Q12_Python',
        'Q12_2': 'Q12_R',
        'Q12_3': 'Q12_SQL',
        'Q12_4': 'Q12_C',
        'Q12_5': 'Q12_C++',
        'Q12_6': 'Q12_Java',
        'Q12_7': 'Q12_Javascript',
        'Q12_8': 'Q12_Julia',
        'Q12_9': 'Q12_Bash',
        'Q12_10': 'Q12_MATLAB',
        'Q12_11': 'Q12_None',
        'Q12_12': 'Q12_Other',
        'Q12_13': 'Q12_C#',
        'Q12_14': 'Q12_PHP',
        'Q12_15': 'Q12_Go',

        'Q13_1': 'Q13_JupyterLab',
        'Q13_2': 'Q13_RStudio',
        'Q13_3': 'Q13_Visual Studio',
        'Q13_4': 'Q13_VSCode',
        'Q13_5': 'Q13_PyCharm',
        'Q13_6': 'Q13_Spyder',
        'Q13_7': 'Q13_Notepad++',
        'Q13_8': 'Q13_Sublime Text',
        'Q13_9': 'Q13_Vim / Emacs',
        'Q13_10': 'Q13_MATLAB',
        'Q13_11': 'Q13_Jupyter Notebook',
        'Q13_12': 'Q13_IntelliJ',
        'Q13_13': 'Q13_None',
        'Q13_14': 'Q13_Other',

        'Q14_1': 'Q14_Kaggle Notebooks',
        'Q14_2': 'Q14_Google Colab',
        'Q14_3': 'Q14_Microsoft Azure Notebooks',
        'Q14_4': 'Q14_Code Ocean',
        'Q14_5': 'Q14_IBM Watson Studio',
        'Q14_6': 'Q14_Amazon SageMaker Studio Notebooks',
        'Q14_7': 'Q14_Amazon SageMaker Studio Lab',
        'Q14_8': 'Q14_Amazon EMR Notebooks',
        'Q14_9': 'Q14_Google Cloud Vertex AI Workbench',
        'Q14_10': 'Q14_Hex Workspaces',
        'Q14_11': 'Q14_Noteable',
        'Q14_12': 'Q14_Databricks Collaborative Notebooks',
        'Q14_13': 'Q14_Deepnote',
        'Q14_14': 'Q14_Gradient Notebooks',
        'Q14_15': 'Q14_None',
        'Q14_16': 'Q14_Other',

        'Q15_1': 'Q15_Matplotlib',
        'Q15_2': 'Q15_Seaborn',
        'Q15_3': 'Q15_Plotly / Dash',
        'Q15_4': 'Q15_Ggplot / ggplot2',
        'Q15_5': 'Q15_Shiny',
        'Q15_6': 'Q15_D3 js',
        'Q15_7': 'Q15_Altair',
        'Q15_8': 'Q15_Bokeh',
        'Q15_9': 'Q15_Geoplotlib',
        'Q15_10': 'Q15_Leaflet / Folium',
        'Q15_11': 'Q15_Pygal',
        'Q15_12': 'Q15_Dygraphs',
        'Q15_13': 'Q15_Highcharter',
        'Q15_14': 'Q15_None',
        'Q15_15': 'Q15_Other',

        'Q17_1': 'Q17_Scikit-learn',
        'Q17_2': 'Q17_TensorFlow',
        'Q17_3': 'Q17_Keras',
        'Q17_4': 'Q17_PyTorch',
        'Q17_5': 'Q17_Fast.ai',
        'Q17_6': 'Q17_XGBoost',
        'Q17_7': 'Q17_LightGBM',
        'Q17_8': 'Q17_CatBoost',
        'Q17_9': 'Q17_Caret',
        'Q17_10': 'Q17_Tidymodels',
        'Q17_11': 'Q17_JAX',
        'Q17_12': 'Q17_PyTorch Lightning',
        'Q17_13': 'Q17_Hugging Face Transformers',
        'Q17_14': 'Q17_None',
        'Q17_15': 'Q17_Other',

        'Q18_1': 'Q18_Linear or Logistic Regression',
        'Q18_2': 'Q18_Decision Trees or Random Forests',
        'Q18_3': 'Q18_Gradient Boosting Machines (xgboost, lightgbm, catboost, etc)',
        'Q18_4': 'Q18_Bayesian Approaches',
        'Q18_5': 'Q18_Evolutionary Approaches',
        'Q18_6': 'Q18_Dense Neural Networks (MLPs, etc)',
        'Q18_7': 'Q18_Convolutional Neural Networks',
        'Q18_8': 'Q18_Generative Adversarial Networks',
        'Q18_9': 'Q18_Recurrent Neural Networks',
        'Q18_10': 'Q18_Transformer Networks (BERT, gpt-3, etc)',
        'Q18_11': 'Q18_Autoencoder Networks',
        'Q18_12': 'Q18_Graph Neural Networks',
        'Q18_13': 'Q18_None',
        'Q18_14': 'Q18_Other',

        'Q19_1': 'Q19_General purpose image/video tools (PIL, OpenCV, etc)',
        'Q19_2': 'Q19_Image segmentation methods (U-Net, Mask R-CNN, etc)',
        'Q19_3': 'Q19_Object detection methods (YOLO, RetinaNet, etc)',
        'Q19_4': 'Q19_Image classification and other general purpose networks (VGG, ResNet, EfficientNet, etc)',
        'Q19_5': 'Q19_Vision transformer networks (ViT, DeiT, etc)',
        'Q19_6': 'Q19_Generative methods (GANs, VAEs, etc)',
        'Q19_7': 'Q19_None',
        'Q19_8': 'Q19_Other',

        'Q20_1': 'Q20_Word embeddings/vectors (GLoVe, word2vec, fastText, etc)',
        'Q20_2': 'Q20_Encoder-decoder models (seq2seq, attention mechanisms)',
        'Q20_3': 'Q20_Contextualized word embeddings (ELMo, CoVe)',
        'Q20_4': 'Q20_Transformer language models (GPT-3, BERT, XLNet, etc)',
        'Q20_5': 'Q20_None',
        'Q20_6': 'Q20_Other',

        'Q21_1': 'Q21_TF Hub',
        'Q21_2': 'Q21_PyTorch Hub',
        'Q21_3': 'Q21_Hugging Face Hub',
        'Q21_4': 'Q21_timm (PyTorch Image Models)',
        'Q21_5': 'Q21_AWS SageMaker JumpStart',
        'Q21_6': 'Q21_ONNX Model Hub',
        'Q21_7': 'Q21_NVIDIA NGC',
        'Q21_8': 'Q21_Kaggle Models',
        'Q21_9': 'Q21_Other publically available model repositories',
        'Q21_10': 'Q21_None',

        'Q28_1': 'Q28_Analyze and understand data to identify business opportunities',
        'Q28_2': 'Q28_Build and/or run the data infrastructure that my team uses',
        'Q28_3': 'Q28_Build prototypes of machine learning models to advance research or business goals',
        'Q28_4': 'Q28_Build and/or run a machine learning service that operationally improves my product or workflows',
        'Q28_5': 'Q28_Improve the performance of existing machine learning models',
        'Q28_6': 'Q28_Do research that advances the field of machine learning',
        'Q28_7': 'Q28_None of these activities are mainly done at work',
        'Q28_8': 'Q28_Other',

        'Q31_1': 'Q31_Amazon Web Services (AWS)',
        'Q31_2': 'Q31_Microsoft Azure',
        'Q31_3': 'Q31_Google Cloud Platform (GCP)',
        'Q31_4': 'Q31_IBM Cloud',
        'Q31_5': 'Q31_Oracle Cloud Infrastructure',
        'Q31_6': 'Q31_SAP Cloud',
        'Q31_7': 'Q31_VMware Cloud',
        'Q31_8': 'Q31_Alibaba Cloud',
        'Q31_9': 'Q31_Tencent Cloud',
        'Q31_10': 'Q31_Huawei Cloud',
        'Q31_11': 'Q31_No / I do not use cloud computing platforms',
        'Q31_12': 'Q31_Other',

        'Q33_1': 'Q33_Amazon EC2',
        'Q33_2': 'Q33_Azure Virtual Machines',
        'Q33_3': 'Q33_Google Compute Engine',
        'Q33_4': 'Q33_No / I do not use virtual machines',
        'Q33_5': 'Q33_Other',

        'Q34_1': 'Q34_Amazon S3',
        'Q34_2': 'Q34_Amazon Elastic File System (EFS)',
        'Q34_3': 'Q34_Google Cloud Storage (GCS)',
        'Q34_4': 'Q34_Google Cloud Filestore',
        'Q34_5': 'Q34_Microsoft Azure Blob Storage',
        'Q34_6': 'Q34_Microsoft Azure Files',
        'Q34_7': 'Q34_No / I do not use cloud storage services',
        'Q34_8': 'Q34_Other',

        'Q35_1': 'Q35_MySQL',
        'Q35_2': 'Q35_PostgreSQL',
        'Q35_3': 'Q35_SQLite',
        'Q35_4': 'Q35_Oracle Database',
        'Q35_5': 'Q35_MongoDB',
        'Q35_6': 'Q35_Snowflake',
        'Q35_7': 'Q35_IBM Db2',
        'Q35_8': 'Q35_Microsoft SQL Server',
        'Q35_9': 'Q35_Amazon Aurora',
        'Q35_10': 'Q35_Amazon Redshift',
        'Q35_11': 'Q35_Amazon RDS',
        'Q35_12': 'Q35_Amazon DynamoDB',
        'Q35_13': 'Q35_Google Cloud BigQuery',
        'Q35_14': 'Q35_Google Cloud SQL',
        'Q35_15': 'Q35_None',
        'Q35_16': 'Q35_Other',

        'Q36_1': 'Q36_Amazon QuickSight',
        'Q36_2': 'Q36_Microsoft Power BI',
        'Q36_3': 'Q36_Google Data Studio',
        'Q36_4': 'Q36_Looker',
        'Q36_5': 'Q36_Tableau',
        'Q36_6': 'Q36_Qlik Sense',
        'Q36_7': 'Q36_Domo',
        'Q36_8': 'Q36_TIBCO Spotfire',
        'Q36_9': 'Q36_Alteryx',
        'Q36_10': 'Q36_Sisense',
        'Q36_11': 'Q36_SAP Analytics Cloud',
        'Q36_12': 'Q36_Azure Synapse Analytics',
        'Q36_13': 'Q36_MicroStrategy',
        'Q36_14': 'Q36_None',
        'Q36_15': 'Q36_Other',

        'Q37_1': 'Q37_Amazon SageMaker',
        'Q37_2': 'Q37_Azure Machine Learning',
        'Q37_3': 'Q37_Google Cloud AI Platform / Vertex AI',
        'Q37_4': 'Q37_DataRobot',
        'Q37_5': 'Q37_Databricks',
        'Q37_6': 'Q37_Dataiku',
        'Q37_7': 'Q37_Alteryx',
        'Q37_8': 'Q37_Rapidminer',
        'Q37_9': 'Q37_C3.ai',
        'Q37_10': 'Q37_Domino Data Lab',
        'Q37_11': 'Q37_H20 AI Cloud',
        'Q37_12': 'Q37_None',
        'Q37_13': 'Q37_Other',

        'Q38_1': 'Q38_Google Cloud AutoML',
        'Q38_2': 'Q38_H20 Driverless AI',
        'Q38_3': 'Q38_Databricks AutoML',
        'Q38_4': 'Q38_DataRobot AutoML',
        'Q38_5': 'Q38_Amazon Sagemaker Autopilot',
        'Q38_6': 'Q38_Azure AutoML',
        'Q38_7': 'Q38_None',
        'Q38_8': 'Q38_Other',

        'Q39_1': 'Q39_TensorFlow Extended (TFX)',
        'Q39_2': 'Q39_TorchServe',
        'Q39_3': 'Q39_ONNX Runtime',
        'Q39_4': 'Q39_Triton Inference Server',
        'Q39_5': 'Q39_OpenVINO Model Server',
        'Q39_6': 'Q39_KServe',
        'Q39_7': 'Q39_BentoML',
        'Q39_8': 'Q39_Multi Model Server (MMS)',
        'Q39_9': 'Q39_Seldon Core',
        'Q39_10': 'Q39_MLflow',
        'Q39_11': 'Q39_Other',
        'Q39_12': 'Q39_None',

        'Q40_1': 'Q40_Neptune.ai',
        'Q40_2': 'Q40_Weights & Biases',
        'Q40_3': 'Q40_Comet.ml',
        'Q40_4': 'Q40_TensorBoard',
        'Q40_5': 'Q40_Guild.ai',
        'Q40_6': 'Q40_ClearML',
        'Q40_7': 'Q40_MLflow',
        'Q40_8': 'Q40_Aporia',
        'Q40_9': 'Q40_Evidently AI',
        'Q40_10': 'Q40_Arize',
        'Q40_11': 'Q40_WhyLabs',
        'Q40_12': 'Q40_Fiddler',
        'Q40_13': 'Q40_DVC',
        'Q40_14': 'Q40_No/None',
        'Q40_15': 'Q40_Other',

        'Q41_1': 'Q41_Google Responsible AI Toolkit',
        'Q41_2': 'Q41_Microsoft Responsible AI Toolbox',
        'Q41_3': 'Q41_IBM AI Ethics tools',
        'Q41_4': 'Q41_Amazon AI Ethics Tools',
        'Q41_5': 'Q41_The LinkedIN Fairness Toolkit',
        'Q41_6': 'Q41_Audit-AI',
        'Q41_7': 'Q41_Aequitas',
        'Q41_8': 'Q41_None',
        'Q41_9': 'Q41_Other',

        'Q42_1': 'Q42_GPUs',
        'Q42_2': 'Q42_TPUs',
        'Q42_3': 'Q42_IPUs',
        'Q42_4': 'Q42_WSEs',
        'Q42_5': 'Q42_RDUs',
        'Q42_6': 'Q42_Trainium Chips',
        'Q42_7': 'Q42_Inferentia Chips',
        'Q42_8': 'Q42_None',
        'Q42_9': 'Q42_Other',

        'Q44_1': 'Q44_Twitter',
        'Q44_2': 'Q44_Email newsletters',
        'Q44_3': 'Q44_Reddit',
        'Q44_4': 'Q44_Kaggle',
        'Q44_5': 'Q44_Course Forums',
        'Q44_6': 'Q44_Youtube',
        'Q44_7': 'Q44_Podcasts',
        'Q44_8': 'Q44_Blogs',
        'Q44_9': 'Q44_Journal Publications',
        'Q44_10': 'Q44_Slack Communities',
        'Q44_11': 'Q44_None',
        'Q44_12': 'Q44_Other'
    }
    df = df.rename(columns=column_name_mapping)

    return df

df = rename_multiple_choice_columns(df)

df.head()

def transform_salary_column(df):

    # Drop rows with missing values in Q29
    df = df.dropna(subset=['Q29']).copy()

    # Function to calculate the midpoint of the salary range
    def calculate_midpoint(salary_range):
        if isinstance(salary_range, str):
            parts = salary_range.split('-')
            if len(parts) == 2:
                try:
                    lower_bound = float(parts[0].replace(',', ''))
                    upper_bound = float(parts[1].replace(',', ''))
                    return (lower_bound + upper_bound) / 2
                except ValueError:
                    return None  # Handle cases where conversion fails
            else:
                return None  # Handle cases where the string format is incorrect
        else:
            return None  # Handle cases where the value is not a string

    # Apply the function to the 'Q29' column and create a new 'Salary' column
    df['Salary'] = df['Q29'].apply(calculate_midpoint)

    # Convert the 'Salary' column to numeric
    df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')

    # Drop rows where 'Salary' is NaN after conversion
    df = df.dropna(subset=['Salary']).copy()

    return df

df = transform_salary_column(df)

print(df[['Q29', 'Salary']].head())
print(df['Salary'].dtype)

def encode_education(df):
    education_mapping = {
        'Professional doctorate': 6,
        'Doctoral degree': 5,
        'Master’s degree': 4,
        'Bachelor’s degree': 3,
        'Some college/university study without earning a bachelor’s degree': 2,
        'No formal education past high school': 1,
        'I prefer not to answer': 0
    }

    df['Education'] = df['Q8'].map(education_mapping)
    df = df.dropna(subset=['Education']).copy() # Drop rows with NaN in 'Education' after mapping
    df['Education'] = df['Education'].astype(int) # Ensure it's numeric

    return df

df = encode_education(df)

print(df[['Q8', 'Education']].head())
print(df['Education'].dtype)

def encode_coding_experience(df):

    coding_exp_mapping = {
        'I have never written code': 0,
        '< 1 year': 1,
        '1-3 years': 2,
        '3-5 years': 3,
        '5-10 years': 4,
        '10-20 years': 5,
        '20+ years': 6
    }

    df['Coding_Experience'] = df['Q11'].map(coding_exp_mapping)
    df = df.dropna(subset=['Coding_Experience']).copy()  # Drop rows with NaN
    df['Coding_Experience'] = df['Coding_Experience'].astype(int)  # Ensure numeric

    return df

df = encode_coding_experience(df)

print(df[['Q11', 'Coding_Experience']].head())
print(df['Coding_Experience'].dtype)

def encode_ml_experience(df):

    ml_experience_mapping = {
        'I do not use machine learning methods': 0,
        'Under 1 year': 1,
        '1-2 years': 2,
        '2-3 years': 3,
        '3-4 years': 4,
        '4-5 years': 5,
        '5-10 years': 6,
        '10-20 years': 7,
        '20 or more years': 8
    }

    df['ML_Experience'] = df['Q16'].map(ml_experience_mapping)
    df = df.dropna(subset=['ML_Experience']).copy()
    df['ML_Experience'] = df['ML_Experience'].astype(int)

    return df

df = encode_ml_experience(df)

print(df[['Q16', 'ML_Experience']].head())
print(df['ML_Experience'].dtype)

def encode_company_size(df):
    company_size_mapping = {
        '0-49 employees': 0,
        '50-249 employees': 1,
        '250-999 employees': 2,
        '1,000-9,999 employees': 3,
        '10,000 or more employees': 4
    }

    df['Company_Size'] = df['Q25'].map(company_size_mapping)
    df = df.dropna(subset=['Company_Size']).copy()
    df['Company_Size'] = df['Company_Size'].astype(int)

    return df

df = encode_company_size(df)

print(df[['Q25', 'Company_Size']].head())
print(df['Company_Size'].dtype)

def encode_data_science_team_size(df):
    team_size_mapping = {
        '0': 0,
        '1-2': 1,
        '3-4': 2,
        '5-9': 3,
        '10-14': 4,
        '15-19': 5,
        '20+': 6
    }

    df['Data_Science_Team_Size'] = df['Q26'].map(team_size_mapping)
    df = df.dropna(subset=['Data_Science_Team_Size']).copy()
    df['Data_Science_Team_Size'] = df['Data_Science_Team_Size'].astype(int)

    return df

df = encode_data_science_team_size(df)

print(df[['Q26', 'Data_Science_Team_Size']].head())
print(df['Data_Science_Team_Size'].dtype)

def encode_ml_cloud_spending(df):
    spending_mapping = {
        '$0 ($USD)': 0,
        '$1-$99': 1,
        '$100-$999': 2,
        '$1,000-$9,999': 3,
        '$10,000-$99,999': 4,
        '$100,000 or more ($USD)': 5
    }

    df['ML_Cloud_Spending'] = df['Q30'].map(spending_mapping)
    df = df.dropna(subset=['ML_Cloud_Spending']).copy()
    df['ML_Cloud_Spending'] = df['ML_Cloud_Spending'].astype(int)

    return df

df = encode_ml_cloud_spending(df)

print(df[['Q30', 'ML_Cloud_Spending']].head())
print(df['ML_Cloud_Spending'].dtype)

def encode_tpu_usage(df):
    tpu_usage_mapping = {
        'Never': 0,
        'Once': 1,
        '2-5 times': 2,
        '6-25 times': 3,
        'More than 25 times': 4
    }

    df['TPU_Usage'] = df['Q43'].map(tpu_usage_mapping)
    df = df.dropna(subset=['TPU_Usage']).copy()
    df['TPU_Usage'] = df['TPU_Usage'].astype(int)

    return df

df = encode_tpu_usage(df)

print(df[['Q43', 'TPU_Usage']].head())
print(df['TPU_Usage'].dtype)

def encode_age(df):

    age_mapping = {
        '18-21': 0,
        '22-24': 1,
        '25-29': 2,
        '30-34': 3,
        '35-39': 4,
        '40-44': 5,
        '45-49': 6,
        '50-54': 7,
        '55-59': 8,
        '60-69': 9,
        '70+': 10
    }

    df['Age'] = df['Q2'].map(age_mapping)
    df = df.dropna(subset=['Age']).copy()
    df['Age'] = df['Age'].astype(int)
    return df

df = encode_age(df)

print(df[['Q2', 'Age']].head())
print(df['Age'].dtype)

columns_to_drop = [
       'Q2', 'Q8', 'Q11', 'Q16', 'Q25', 'Q26', 'Q29', 'Q30', 'Q43'
    ]
df = df.drop(columns=columns_to_drop)

def convert_duration_to_numeric(df):
    # Convert the column to numeric, errors='coerce' will replace non-numeric values with NaN
    df['Duration (in seconds)'] = pd.to_numeric(df['Duration (in seconds)'], errors='coerce')

    # Drop rows where 'Duration (in seconds)' is NaN after conversion
    df = df.dropna(subset=['Duration (in seconds)']).copy()

    return df

df = convert_duration_to_numeric(df)

columns_to_encode = ['Q3', 'Q4', 'Q5', 'Q9', 'Q22', 'Q23', 'Q24', 'Q27', 'Q32']

# Perform one-hot encoding
df = pd.get_dummies(df, columns=columns_to_encode, prefix=columns_to_encode)

# Display the first few rows of the modified DataFrame and the new data types
print(df.head())
print(df.dtypes)

columns_to_drop = [
       'Q2', 'Q8', 'Q11', 'Q16', 'Q25', 'Q26', 'Q29', 'Q30', 'Q43'
    ]
df = df.drop(columns=columns_to_drop)

def convert_duration_to_numeric(df):
    # Convert the column to numeric, errors='coerce' will replace non-numeric values with NaN
    df['Duration (in seconds)'] = pd.to_numeric(df['Duration (in seconds)'], errors='coerce')

    # Drop rows where 'Duration (in seconds)' is NaN after conversion
    df = df.dropna(subset=['Duration (in seconds)']).copy()

    return df

df = convert_duration_to_numeric(df)

object_columns_df = df.select_dtypes(include=['object'])
print(object_columns_df.columns)

# 1. Define relevant tool/tech columns (ensure correctness based on your data)
tool_tech_prefixes = ['Q6','Q7','Q12', 'Q13', 'Q14', 'Q15', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q28', 'Q31', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37','Q38','Q39', 'Q40','Q41','Q42', 'Q44']
tool_tech_cols = []
for prefix in tool_tech_prefixes:
    tool_tech_cols.extend([col for col in df.columns if col.startswith(prefix + '_')])

# Clean and filter data
df_filtered = df[tool_tech_cols + ['Salary']].copy()
df_filtered = df_filtered.dropna(subset=['Salary'])  # Drop rows with NaN in target or key features
df_filtered = df_filtered.fillna(0)  # Fill remaining NaNs with 0 (for tool/tech usage)

# Convert tool/tech columns to numeric (if needed - might already be)
for col in tool_tech_cols:
    df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce').fillna(0)

# --- Model Training ---
from sklearn.linear_model import Lasso

# Prepare data for modeling
X = df_filtered.drop('Salary', axis=1)
y = df_filtered['Salary']

# Scale numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Lasso Regression (adjust alpha as needed)
lasso = Lasso(alpha=10)
lasso.fit(X_train, y_train)

# Get coefficients
coefficients = lasso.coef_

# Map coefficients to feature names
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': abs(coefficients)})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

print("Top Influential Tools/Techniques & Other Factors on Salary:\n")
print(feature_importance.head(20))

# Visualize feature importance
plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))
plt.title('Top 20 Factors Influencing Data Scientist Salaries')
plt.xlabel('Importance (Absolute Lasso Coefficient)')
plt.ylabel('Feature')
plt.show()

# 1. Define relevant tool/tech columns (ensure correctness based on your data)
tool_tech_prefixes = ['Q6','Q7','Q12', 'Q13', 'Q14', 'Q15', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q28', 'Q31', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37','Q38','Q39', 'Q40','Q41','Q42', 'Q44']
tool_tech_cols = []
for prefix in tool_tech_prefixes:
    tool_tech_cols.extend([col for col in df.columns if col.startswith(prefix + '_')])
# Initialize an empty dictionary to store the counts
tool_technique_counts = {}
for col in tool_tech_cols:
    if col in cols:
        # Get the tool/technique name from the column name (after the prefix)
        tool_technique = col.split('_', 1)[1] if '_' in col else col
        # Count the occurrences of the tool/technique (assuming binary encoding)
        count = df[col].astype(int).sum()
        tool_technique_counts[tool_technique] = tool_technique_counts.get(tool_technique, 0) + count

# Convert the dictionary to a DataFrame for easier analysis
tool_technique_df = pd.DataFrame(list(tool_technique_counts.items()), columns=['Tool/Technique', 'Count'])

# Sort by frequency to get the most prevalent ones
tool_technique_df = tool_technique_df.sort_values(by='Count', ascending=False)

print("Prevalent Tools and Techniques in Data Science:\n")
print(tool_technique_df.head(10))  # Show top 10

# 1. What are the most significant factors driving Data Scientist salaries?
# Correlation analysis
correlation_matrix = df.corr()
salary_correlations = correlation_matrix['Salary'].sort_values(ascending=False)
print("\nCorrelations with Salary:")
print(salary_correlations)

# Visualize the top correlations
plt.figure(figsize=(12, 8))
sns.barplot(x=salary_correlations.head(10), y=salary_correlations.head(10).index)
plt.title('Top 10 Factors Correlated with Salary')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Factors')
plt.show()

# 2. What are the most prevalent tools (software) and techniques (methods) being applied?
# Identify tool/technique related columns.  You'll need to adjust this list
# based on the exact column names in your DataFrame.
# Identify tool/technique related columns
tool_technique_cols = [
    'Q6_Coursera', 'Q6_edX', 'Q6_Kaggle Learn Courses', 'Q6_DataCamp',
    'Q6_Fast.ai', 'Q6_Udacity', 'Q6_Udemy', 'Q6_LinkedIn Learning',
    'Q6_Cloud-certification programs (direct from AWS, Azure, GCP, or similar)',
    'Q6_University Courses (resulting in a degree)', 'Q6_Other',
    'Q7_University Courses',
    'Q7_Online courses (Coursera, edX, Kaggle Learn, etc)',
    'Q7_Social media platforms (Reddit, Twitter, etc)',
    'Q7_Video platforms (YouTube, Vimeo, etc)',
    'Q7_Kaggle (notebooks, forums, etc)',
    'Q7_None', 'Q7_Other', 'Q12_Python', 'Q12_R', 'Q12_SQL',
    'Q12_C', 'Q12_C++', 'Q12_Java', 'Q12_Javascript', 'Q12_Julia', 'Q12_Bash',
    'Q12_MATLAB', 'Q12_None', 'Q12_Other', 'Q12_C#', 'Q12_PHP', 'Q12_Go',
    'Q13_JupyterLab', 'Q13_RStudio', 'Q13_Visual Studio', 'Q13_VSCode',
    'Q13_PyCharm', 'Q13_Spyder', 'Q13_Notepad++', 'Q13_Sublime Text',
    'Q13_Vim / Emacs', 'Q13_MATLAB', 'Q13_Jupyter Notebook', 'Q13_IntelliJ',
    'Q13_None', 'Q13_Other', 'Q14_Kaggle Notebooks', 'Q14_Google Colab',
    'Q14_Microsoft Azure Notebooks', 'Q14_Code Ocean', 'Q14_IBM Watson Studio',
    'Q14_Amazon SageMaker Studio Notebooks', 'Q14_Amazon SageMaker Studio Lab',
    'Q14_Amazon EMR Notebooks', 'Q14_Google Cloud Vertex AI Workbench',
    'Q14_Hex Workspaces', 'Q14_Noteable', 'Q14_Databricks Collaborative Notebooks',
    'Q14_Deepnote', 'Q14_Gradient Notebooks', 'Q14_None', 'Q14_Other',
    'Q15_Matplotlib', 'Q15_Seaborn', 'Q15_Plotly / Dash', 'Q15_Ggplot / ggplot2',
    'Q15_Shiny', 'Q15_D3 js', 'Q15_Altair', 'Q15_Bokeh', 'Q15_Geoplotlib',
    'Q15_Leaflet / Folium', 'Q15_Pygal', 'Q15_Dygraphs', 'Q15_Highcharter',
    'Q15_None', 'Q15_Other', 'Q17_Scikit-learn', 'Q17_TensorFlow', 'Q17_PyTorch',
    'Q17_Keras', 'Q17_Fast.ai', 'Q17_XGBoost', 'Q17_LightGBM',
    'Q17_CatBoost', 'Q17_Caret', 'Q17_Tidymodels', 'Q17_JAX', 'Q17_PyTorch Lightning',
    'Q17_Hugging Face Transformers', 'Q17_None', 'Q17_Other', 'Q18_Linear or Logistic Regression', 'Q18_Decision Trees or Random Forests',
    'Q18_Gradient Boosting Machines (xgboost, lightgbm, catboost, etc)','Q18_Bayesian Approaches', 'Q18_Evolutionary Approaches',
    'Q18_Dense Neural Networks (MLPs, etc)','Q18_Convolutional Neural Networks','Q18_Generative Adversarial Networks',
    'Q18_Recurrent Neural Networks','Q18_Transformer Networks (BERT, gpt-3, etc)','Q18_Autoencoder Networks',
    'Q18_Graph Neural Networks', 'Q18_None', 'Q18_Other','Q19_General purpose image/video tools (PIL, OpenCV, etc)',
    'Q19_Image segmentation methods (U-Net, Mask R-CNN, etc)','Q19_Object detection methods (YOLO, RetinaNet, etc)',
    'Q19_Image classification and other general purpose networks (VGG, ResNet, EfficientNet, etc)', 'Q19_Vision transformer networks (ViT, DeiT, etc)',
    'Q19_Generative methods (GANs, VAEs, etc)', 'Q19_None', 'Q19_Other', 'Q20_Word embeddings/vectors (GLoVe, word2vec, fastText, etc)',
    'Q20_Encoder-decoder models (seq2seq, attention mechanisms)',
    'Q20_Contextualized word embeddings (ELMo, CoVe)',
    'Q20_Transformer language models (GPT-3, BERT, XLNet, etc)','Q20_None','Q20_Other',
    'Q21_TF Hub','Q21_PyTorch Hub',
       'Q21_Hugging Face Hub',
       'Q21_timm (PyTorch Image Models)',
       'Q21_AWS SageMaker JumpStart', 'Q21_ONNX Model Hub','Q21_NVIDIA NGC',
       'Q21_Kaggle Models','Q21_Other publically available model repositories','Q21_None',
    'Q28_Analyze and understand data to identify business opportunities',
       'Q28_Build and/or run the data infrastructure that my team uses',
       'Q28_Build prototypes of machine learning models to advance research or business goals',
       'Q28_Build and/or run a machine learning service that operationally improves my product or workflows',
       'Q28_Improve the performance of existing machine learning models',
       'Q28_Do research that advances the field of machine learning',
       'Q28_None of these activities are mainly done at work',
       'Q28_Other',
    'Q31_Amazon Web Services (AWS)',
       'Q31_Microsoft Azure',
       'Q31_Google Cloud Platform (GCP)',
        'Q31_IBM Cloud',
       'Q31_Oracle Cloud Infrastructure',
       'Q31_SAP Cloud',
        'Q31_VMware Cloud',
        'Q31_Alibaba Cloud',
       'Q31_Tencent Cloud',
       'Q31_Huawei Cloud',
        'Q31_No / I do not use cloud computing platforms',
       'Q31_Other',
        'Q33_Azure Virtual Machines',
       'Q33_Google Compute Engine',
        'Q33_No / I do not use virtual machines',
        'Q33_Other', 'Q34_Amazon S3',
       'Q34_Amazon Elastic File System (EFS)',
       'Q34_Google Cloud Storage (GCS)',
       'Q34_Google Cloud Filestore',
       'Q34_Microsoft Azure Blob Storage',
       'Q34_Microsoft Azure Files',
       'Q34_No / I do not use cloud storage services',
       'Q34_Other',
    'Q35_MySQL',
       'Q35_PostgreSQL',
       'Q35_SQLite',
       'Q35_Oracle Database',
       'Q35_MongoDB',
       'Q35_Snowflake',
       'Q35_IBM Db2',
       'Q35_Microsoft SQL Server',
       'Q35_Amazon Aurora',
       'Q35_Amazon Redshift',
       'Q35_Amazon RDS',
       'Q35_Amazon DynamoDB',
       'Q35_Google Cloud BigQuery',
       'Q35_Google Cloud SQL',
       'Q35_None',
       'Q35_Other', 'Q36_Amazon QuickSight', 'Q36_Microsoft Power BI',
    'Q36_Google Data Studio', 'Q36_Looker', 'Q36_Tableau', 'Q36_Qlik Sense', 'Q36_Domo',
    'Q36_TIBCO Spotfire', 'Q36_Alteryx','Q36_Sisense',
       'Q36_SAP Analytics Cloud','Q36_Azure Synapse Analytics','Q36_MicroStrategy',
       'Q36_None','Q36_Other',
    'Q37_Amazon SageMaker','Q37_Azure Machine Learning',
       'Q37_Google Cloud AI Platform / Vertex AI', 'Q37_DataRobot',
       'Q37_Databricks','Q37_Dataiku','Q37_Alteryx','Q37_Rapidminer',
       'Q37_C3.ai','Q37_Domino Data Lab',
       'Q37_H20 AI Cloud','Q37_None',
       'Q37_Other',
    'Q38_Google Cloud AutoML','Q38_H20 Driverless AI','Q38_Databricks AutoML',
       'Q38_DataRobot AutoML','Q38_Amazon Sagemaker Autopilot','Q38_Azure AutoML','Q38_None',
       'Q38_Other',
    'Q39_TensorFlow Extended (TFX)','Q39_TorchServe','Q39_ONNX Runtime',
       'Q39_Triton Inference Server','Q39_OpenVINO Model Server','Q39_KServe',
       'Q39_BentoML','Q39_Multi Model Server (MMS)','Q39_Seldon Core','Q39_MLflow',
       'Q39_Other','Q39_None',
    'Q40_Neptune.ai','Q40_Weights & Biases','Q40_Comet.ml','Q40_TensorBoard',
       'Q40_Guild.ai','Q40_ClearML','Q40_MLflow','Q40_Aporia','Q40_Evidently AI',
       'Q40_Arize','Q40_WhyLabs','Q40_Fiddler','Q40_DVC','Q40_No/None','Q40_Other',
    'Q41_Google Responsible AI Toolkit','Q41_Microsoft Responsible AI Toolbox',
       'Q41_IBM AI Ethics tools', 'Q41_Amazon AI Ethics Tools','Q41_The LinkedIN Fairness Toolkit',
       'Q41_Audit-AI', 'Q41_Aequitas', 'Q41_None', 'Q41_Other',
    'Q42_GPUs','Q42_TPUs','Q42_IPUs','Q42_WSEs','Q42_RDUs','Q42_Trainium Chips',
       'Q42_Inferentia Chips','Q42_None','Q42_Other',
    'Q44_Twitter','Q44_Email newsletters','Q44_Reddit','Q44_Kaggle',
      'Q44_Course Forums','Q44_Youtube', 'Q44_Podcasts', 'Q44_Blogs','Q44_Journal Publications',
       'Q44_Slack Communities','Q44_None','Q44_Other'
]

# Calculate usage frequency (mean) for each tool/technique
tool_usage = df[tool_technique_cols].mean().sort_values(ascending=False)
print("\nMost Prevalent Tools and Techniques:")
print(tool_usage.head(10))

# Visualize
plt.figure(figsize=(12, 8))
sns.barplot(x=tool_usage.head(10), y=tool_usage.head(10).index)
plt.title('Top 10 Most Prevalent Tools and Techniques')
plt.xlabel('Usage Frequency')
plt.ylabel('Tool/Technique')
plt.show()

# 3. What tools and techniques are emerging?
# You might define "emerging" as tools with relatively lower overall usage
# but significant recent growth (if you had year-over-year data).
# For this example, let's just look at the least used:
emerging_tools = tool_usage.tail(20)  # Bottom 10
print("\nEmerging Tools and Techniques (Least Used in this survey):")
print(emerging_tools)

plt.figure(figsize=(12, 8))
sns.barplot(x=emerging_tools.values, y=emerging_tools.index)
plt.title('Emerging Tools and Techniques (Least Used)')
plt.xlabel('Usage Frequency')
plt.ylabel('Tool/Technique')
plt.show()

# List of multiple-choice column prefixes related to tools and techniques
tool_technique_prefixes = ['Q6','Q7','Q12', 'Q13', 'Q14', 'Q15', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q28', 'Q31', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37','Q38','Q39', 'Q40','Q41','Q42', 'Q44']

# Dictionary to store the frequency of each tool and technique
tool_technique_counts = {}

for prefix in tool_technique_prefixes:
    # Get all columns that start with the prefix
    cols = [col for col in df.columns if col.startswith(prefix + '_') or col == prefix]

    for col in cols:
        # Get the tool/technique name from the column name (after the prefix)
        tool_technique = col.split('_', 1)[1] if '_' in col else col
        # Count the occurrences of the tool/technique (assuming binary encoding)
        count = df[col].astype(int).sum()
        tool_technique_counts[tool_technique] = tool_technique_counts.get(tool_technique, 0) + count

# Convert the dictionary to a DataFrame for easier analysis
tool_technique_df = pd.DataFrame(list(tool_technique_counts.items()), columns=['Tool/Technique', 'Count'])

# Sort by frequency to get the most prevalent ones
tool_technique_df = tool_technique_df.sort_values(by='Count', ascending=False)

print("Emerging Tools and Techniques in Data Science:\n")
print(tool_technique_df)

def plot_multi_choice(df, prefix, title, top_n=10):
    cols = [col for col in df.columns if col.startswith(prefix)]
    summary = df[cols].sum().sort_values(ascending=False).head(top_n)
    summary.index = [col.split('_')[-1] for col in summary.index]  # simplify labels
    summary.plot(kind='bar', figsize=(10, 5), title=title)
    plt.ylabel("Number of Selections")
    plt.xlabel("Choice")
    plt.grid(True)
    plt.show()

# Q6 - Platforms for learning
plot_multi_choice(df, 'Q6_', 'Where Data Scientists Learn: Popular Platforms')

# Q7 - Most helpful when starting
plot_multi_choice(df, 'Q7_', 'Most Helpful Tools for Beginners in Data Science')

# Q12 - Tools used regularly
plot_multi_choice(df, 'Q12_', 'ML & DS Tools Used Regularly')

# Q15 - Programming Languages
plot_multi_choice(df, 'Q15_', 'Programming Languages Regularly Used')

# Q28 - Core Job Activities
plot_multi_choice(df, 'Q28_', 'Common Work Activities in Data Science Roles')

# Q44 - Trusted Data Science Media
plot_multi_choice(df, 'Q44_', 'Top Media Sources for Data Science News & Learning')

from sklearn.linear_model import LassoCV

# Filter rows with valid salary (Q29)
df_filtered = df[df['Salary'].notna()].copy()

# Encode salary (Q29) as numeric if not already
df_filtered['Salary'] = pd.to_numeric(df_filtered['Salary'], errors='coerce')

# Drop rows with NaN salaries after conversion
df_filtered = df_filtered.dropna(subset=['Salary'])

# Combine all feature sets
question_prefixes = ['Q6_', 'Q7_', 'Q12_', 'Q15_', 'Q28_', 'Q44_']
feature_cols = [col for col in df_filtered.columns if any(col.startswith(q) for q in question_prefixes)]

# Prepare X and y
X = df_filtered[feature_cols].fillna(0).astype(int)  # binary features
y = df_filtered['Salary']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Lasso regression with cross-validation
lasso = LassoCV(cv=5, random_state=0)
lasso.fit(X_train, y_train)

# Create a DataFrame of feature importances
importance = pd.Series(lasso.coef_, index=X.columns)
importance = importance[importance != 0].sort_values(key=np.abs, ascending=False)

# Plot the most influential features
plt.figure(figsize=(10, 6))
importance.head(15).plot(kind='barh')
plt.title("Top Features Predicting Compensation (Lasso Regression)")
plt.xlabel("Coefficient (importance)")
plt.gca().invert_yaxis()
plt.grid(True)
plt.tight_layout()
plt.show()

df_model = df.copy()

df_model = df_model[df_model['Salary'].notna() & df_model['Education'].notna()].copy()
df_model['Salary'] = pd.to_numeric(df_model['Salary'], errors='coerce')
df_model = df_model.dropna(subset=['Salary'])

# Define alternative learning features
alt_learning_prefixes = ['Q6_', 'Q7_', 'Q44_']
alt_learning_cols = [col for col in df_model if any(col.startswith(p) for p in alt_learning_prefixes)]

# Final feature set
X = df_model[['Education'] + alt_learning_cols].fillna(0).astype(float)
y = df_model['Salary']

# Standardize and split
X_scaled = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)

# Run LassoCV
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_train, y_train)

# Feature importances
importance = pd.Series(lasso.coef_, index=X.columns)
importance = importance[importance != 0].sort_values(key=np.abs, ascending=False)

# Plot
plt.figure(figsize=(10, 6))
importance.head(15).plot(kind='barh')
plt.title("Formal vs Alternative Education: Predictors of Compensation")
plt.xlabel("Coefficient (importance)")
plt.gca().invert_yaxis()
plt.grid(True)
plt.tight_layout()
plt.show()

import pickle
# Save the trained model as a pickle file
filename = '/content/data_scientist_salary2025_model.pkl'
pickle.dump(lasso, open(filename, 'wb'))

# Load and test the saved model (optional)
loaded_model = pickle.load(open(filename, 'rb'))